{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "403b90b536e46321",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Proyecto Open Data I\n",
    "## Radares, y su eficiencia en la CAM\n",
    "### Recopilación, limpieza y tratamiento de los datos\n",
    "Este cuaderno pretende enseñar el proceso de limpieza de los datos relativos a los radares en la CAM\n",
    "_Paula Gómez Lucas, Alejandro Majado Martínez_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d3e82a2b2af617",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Importar librerías\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pandas_summary import DataFrameSummary\n",
    "import textwrap\n",
    "\n",
    "from scipy.stats import chisquare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae30c35b38b2cfe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A continuación, se muestra la clase que está compuesta de todos los métodos que se encargan de la limpieza y transformación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d478fe7f47cd57",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CSVDataLoader:\n",
    "    \"\"\"\n",
    "    A class for loading and cleaning CSV data from a specified folder path.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    folder_path : str\n",
    "        The path to the folder containing the CSV files to be loaded.\n",
    "\n",
    "    data : dict\n",
    "        A dictionary containing the loaded CSV data, where the keys are the file names and the values are the corresponding dataframes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, folder_path):\n",
    "        \"\"\"\n",
    "        Initializes a CSVDataLoader object with the specified folder path.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        folder_path : str\n",
    "            The path to the folder containing the CSV files to be loaded.\n",
    "        \"\"\"\n",
    "        self.folder_path = folder_path\n",
    "        self.data = {}\n",
    "        self.filename = []\n",
    "        self.keys = []\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Loads CSV data from the specified folder path into a dictionary.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        None\n",
    "        \"\"\"\n",
    "        csv_files = [f for f in os.listdir(self.folder_path) if f.endswith('.csv')]\n",
    "        folders = (\"datasets/actuacionesBomberos\", \"datasets/estaciones\", \"datasets/accidentalidad\")\n",
    "        for folder in folders:\n",
    "            df = None\n",
    "            for file in os.listdir(folder):\n",
    "                filepath = folder + \"/\" + file\n",
    "                df1 = pd.read_csv(filepath, sep=';', encoding='utf-8', low_memory=False)\n",
    "                df = pd.concat([df, df1])\n",
    "            self.data[str(folder)] = df\n",
    "\n",
    "        for file_name in csv_files:\n",
    "            file_path = os.path.join(self.folder_path, file_name)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, sep=';', encoding='latin-1', low_memory=False)\n",
    "                self.data[str(file_name)] = df\n",
    "                self.filename.append(file_name)\n",
    "            except Exception as e:\n",
    "                print(f\"Error al leer {file_name}: {str(e)}\")\n",
    "\n",
    "        for value in self.data.keys():\n",
    "            self.keys.append(value)\n",
    "\n",
    "    def clean_data(self):\n",
    "        \"\"\"\n",
    "        Cleans the loaded CSV data by renaming columns, removing whitespace, dropping null values and duplicates, and converting date columns to datetime format.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        None\n",
    "        \"\"\"\n",
    "        columna_borrar = \"Unnamed\"\n",
    "        for df in self.data:\n",
    "            for j in self.data[df].columns:\n",
    "                if columna_borrar in j:\n",
    "                    while j in self.data[df].columns:\n",
    "                        self.data[df] = self.data[df].drop(j, axis=1)\n",
    "                        self.data[df] = self.data[df].dropna(how='all', axis=0)\n",
    "                        \n",
    "            self.data[df] = self.data[df].rename(columns = lambda x: x.strip().lower().replace(' ', '_'))\n",
    "            self.data[df] = self.data[df].map(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "            self.data[df] = self.data[df].dropna(how='all', axis=0)\n",
    "            self.data[df] = self.data[df].drop_duplicates()\n",
    "            self.data[df] = self.data[df].loc[:, ~self.data[df].columns.duplicated()]\n",
    "            self.data[df].columns = map(str.upper, self.data[df].columns)\n",
    "\n",
    "            if 'FECHA' in self.data[df].columns:\n",
    "                self.data[df]['FECHA'] = pd.to_datetime(self.data[df]['FECHA'], format='%d/%m/%Y')\n",
    "\n",
    "#           num_cols = self.data[i].select_dtypes(include='number').columns\n",
    "#           for col in num_cols:\n",
    "#               self.data[i][col] = self.data[i][col].fillna(self.data[i][col].mean())\n",
    "\n",
    "    def get_info(self, filename):\n",
    "        print(self.data[filename].isnull().sum())\n",
    "        print(self.data[filename].info())\n",
    "        \n",
    "    def get_nan_columns(self):\n",
    "        j = 0\n",
    "        for i in self.data:\n",
    "            print(self.keys[j])\n",
    "            self.get_info(i)\n",
    "            j+=1\n",
    "      \n",
    "    def get_cleaned_data(self):\n",
    "        \"\"\"\n",
    "        Returns the cleaned CSV data as a dictionary.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            A dictionary containing the cleaned CSV data, where the keys are the file names and the values are the corresponding dataframes.\n",
    "        \"\"\"\n",
    "        return self.data\n",
    "\n",
    "    def create_graph(df, colummn, name):\n",
    "        frec = df[''+str(colummn)].value_counts()\n",
    "        aux_df = pd.DataFrame(frec)\n",
    "        aux_df.columns = [\"Frecuencia absoluta\"]\n",
    "        aux_df[\"Frecuencia relativa\"] = 100*aux_df[\"Frecuencia absoluta\"] / len(df)\n",
    "        frec_rel_cumsum = aux_df[\"Frecuencia relativa\"].cumsum()\n",
    "        aux_df[\"Frecuencia relativa acumulada\"] = frec_rel_cumsum\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "        ax.set_title('Distribución de '+ str(name))\n",
    "        ax.bar(aux_df.index, aux_df['Frecuencia absoluta'], color='blue')\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(aux_df.index, aux_df['Frecuencia relativa acumulada'], color='red', marker='o', ms = 5)\n",
    "        ax2.yaxis.set_major_formatter(PercentFormatter())\n",
    "        ax.tick_params(axis='y', color = 'blue')\n",
    "        ax2.tick_params(axis='y', color = 'red')\n",
    "        ax.set_xticklabels(aux_df.index, rotation=90)\n",
    "        plt.show()\n",
    "\n",
    "    def dataframe_summary(self, filename):\n",
    "\n",
    "        numeric_mask = self.data[filename].select_dtypes(include='number').columns\n",
    "\n",
    "        # Create a DataFrameSummary object\n",
    "        summary = DataFrameSummary(self.data[filename][numeric_mask])\n",
    "    \n",
    "        # Display summary statistics\n",
    "        summary_stats = summary.summary()\n",
    "    \n",
    "        # Plot correlation matrix for all numeric columns\n",
    "        self.data[filename][numeric_mask].corr(method='pearson', numeric_only=True)\n",
    "\n",
    "        # Boxplot of each variable\n",
    "        #self.data[filename][numeric_mask].boxplot(figsize=(10, 8))\n",
    "\n",
    "        return summary_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd85edb70ad193c8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Una vez está definida la clase con sus métodos, procedemos a declarar las variables que nos permiten trabajar con ello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283a71997af30bfb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "folder_path = \"datasets\"\n",
    "data_loader = CSVDataLoader(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63352d6285012f0e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "La siguiente función carga los datos de los csv a los dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665ff0b29c363cd8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_loader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa768687373638fb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Limpiamos los datos eliminando las columnas autogeneradas con NaNs, renombramos las columnas para que sean uniformes (minúsculas y con barra bajas), eliminamos las filas de NaNs, eliminamos las filas duplicadas, formateamos todas las variables fecha para que sean consistentes (dd/mm/aaaa), sustituimos los NaNs de las variables numéricas con la media correspondiente a su variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f338f27e9e6e88",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_loader.clean_data()\n",
    "data = data_loader.get_cleaned_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4457e965ecfd44",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Por último, para confirmar que los datos se han cargado bien, utilizamos el método get_nan_columns para ver cuántos datos en cada columna quedan nulos, así como usamos el método info() de pandas para ver un resumen de todas las columnas y comprobamos también que todo el formateo de las columnas se ha realizado sin problema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861c0394be81dc0e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Empecemos con accidentalidad: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587e7d49e462e29",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_loader.get_info('datasets/accidentalidad')\n",
    "data_loader.data['datasets/accidentalidad']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a3185975249614",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Hay algunos datos faltantes que tiene sentido que lo sean, y podemos sustituir por un buzzword de algún tipo que nos haga saber que se trate de esto, como lo es que en un accidente en el que no se han producido lesiones, la lesividad sea nula y tampoco haya código de la misma, y como ambas cifras coinciden, es lógico pensar que se trata de las mismas situaciones. Podemos sustituir entonces todos los NaNs de Lesividad faltantes por 'Se desconoce' y el código por 77."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2fcca273dee177",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_loader.data['datasets/accidentalidad']['COD_LESIVIDAD'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cb2e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader.data['datasets/accidentalidad']['LESIVIDAD'].where(data_loader.data['datasets/accidentalidad']['COD_LESIVIDAD']==1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db73e37a9c97118d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_loader.data['datasets/accidentalidad']['LESIVIDAD'].where(data_loader.data['datasets/accidentalidad']['COD_LESIVIDAD']==2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd5d7f494a15bd3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_loader.data['datasets/accidentalidad']['LESIVIDAD'].where(data_loader.data['datasets/accidentalidad']['COD_LESIVIDAD']==3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd88d206769050c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_loader.data['datasets/accidentalidad']['LESIVIDAD'].where(data_loader.data['datasets/accidentalidad']['COD_LESIVIDAD']==4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc004c773d928874",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_loader.data['datasets/accidentalidad']['LESIVIDAD'].where(data_loader.data['datasets/accidentalidad']['COD_LESIVIDAD']==5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c0af8f7e1dffe4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_loader.data['datasets/accidentalidad']['LESIVIDAD'].where(data_loader.data['datasets/accidentalidad']['COD_LESIVIDAD']==6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9313599b81855c77",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_loader.data['datasets/accidentalidad']['LESIVIDAD'].where(data_loader.data['datasets/accidentalidad']['COD_LESIVIDAD']==7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb90af5352dc3e6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_loader.data['datasets/accidentalidad']['LESIVIDAD'].where(data_loader.data['datasets/accidentalidad']['COD_LESIVIDAD']==14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d61119ab1b9a45",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_loader.data['datasets/accidentalidad']['LESIVIDAD'].where(data_loader.data['datasets/accidentalidad']['COD_LESIVIDAD']==77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ad54e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping dictionary\n",
    "mapping_dict = {\n",
    "    1: 'Atención en urgencias sin posterior ingreso',\n",
    "    2: 'Ingreso inferior o igual a 24 horas',\n",
    "    3: 'Ingreso superior a 24 horas',\n",
    "    4: 'Fallecido 24 horas',\n",
    "    5: 'Asistencia sanitaria ambulatoria con posterioridad',\n",
    "    6: 'Asistencia sanitaria inmediata en centro de salud o mutua',\n",
    "    7: 'Asistencia sanitaria sólo en el lugar del accidente',\n",
    "    14: 'Sin asistencia sanitaria',\n",
    "    77: 'Se desconoce',\n",
    "}\n",
    "\n",
    "# Fill missing values using the mapping dictionary\n",
    "data_loader.data['datasets/accidentalidad']['LESIVIDAD'] = data_loader.data['datasets/accidentalidad']['LESIVIDAD'].fillna(data_loader.data['datasets/accidentalidad']['COD_LESIVIDAD'].map(mapping_dict))\n",
    "data_loader.data['datasets/accidentalidad']['COD_LESIVIDAD'] = data_loader.data['datasets/accidentalidad']['COD_LESIVIDAD'].fillna(data_loader.data['datasets/accidentalidad']['LESIVIDAD'].map({v: k for k, v in mapping_dict.items()}))\n",
    "\n",
    "data_loader.data['datasets/accidentalidad']['LESIVIDAD'].fillna('Se desconoce', inplace=True)\n",
    "data_loader.data['datasets/accidentalidad']['COD_LESIVIDAD'].fillna(77, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6a142e6af20ddc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Por otro lado, positivo en droga tiene valor sólo si daba positivo, por lo que rellenar los valores faltantes con 0 es lo más lógico (siendo 0 negativo en droga). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0770e842c6602b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_loader.data['datasets/accidentalidad']['POSITIVA_DROGA'] = data_loader.data['datasets/accidentalidad']['POSITIVA_DROGA'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfef480eb314978e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Número, código de distrito, tipo de accidente, coordenadas (x e y), son atributos a los que sólo les falta un dato cada uno, por lo que no es representativo esta falta de datos y podemos rellenarlos con el valor más habitual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523ced2bbb542e56",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numero = data_loader.data['datasets/accidentalidad']['COD_DISTRITO'].mode().iat[0]\n",
    "codDistrito = data_loader.data['datasets/accidentalidad']['COD_DISTRITO'].mode().iat[0]\n",
    "distrito = data_loader.data['datasets/accidentalidad']['COD_DISTRITO'].mode().iat[0]\n",
    "accidente = data_loader.data['datasets/accidentalidad']['COD_DISTRITO'].mode().iat[0]\n",
    "coorX = data_loader.data['datasets/accidentalidad']['COD_DISTRITO'].mode().iat[0]\n",
    "coorY = data_loader.data['datasets/accidentalidad']['COD_DISTRITO'].mode().iat[0]\n",
    "\n",
    "\n",
    "data_loader.data['datasets/accidentalidad']['NUMERO'] = data_loader.data['datasets/accidentalidad']['NUMERO'].fillna(numero)\n",
    "data_loader.data['datasets/accidentalidad']['COD_DISTRITO'] = data_loader.data['datasets/accidentalidad']['COD_DISTRITO'].fillna(codDistrito)\n",
    "data_loader.data['datasets/accidentalidad']['DISTRITO'] = data_loader.data['datasets/accidentalidad']['DISTRITO'].fillna(distrito)\n",
    "data_loader.data['datasets/accidentalidad']['TIPO_ACCIDENTE'] = data_loader.data['datasets/accidentalidad']['TIPO_ACCIDENTE'].fillna(accidente)\n",
    "data_loader.data['datasets/accidentalidad']['COORDENADA_X_UTM'] = data_loader.data['datasets/accidentalidad']['COORDENADA_X_UTM'].fillna(coorX)\n",
    "data_loader.data['datasets/accidentalidad']['COORDENADA_Y_UTM'] = data_loader.data['datasets/accidentalidad']['COORDENADA_Y_UTM'].fillna(coorY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ffcefbaed71bb7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Por último, donde queda dilema es en positivo alcohol, tipo de vehículo y estado meteorológico. En esta situación, lo más apropiado es ver si, relacionando estos atributos con algún otro, es más probable que los atributos valgan uno u otro valor.\n",
    "\n",
    "- Estado meteorológico. Hay 7 valores posibles: despejado, lluvia débil, lluvia intensa, granizando, nevando, nublado, se desconoce. Aquí, por lo tanto, hay 3 vías de actuación:\n",
    "    - Rellenar con \"se desconoce\", i.e.: ser fieles a lo que se sabe, reducir la proporción de datos artificiales (hay un 11% de datos faltantes), solución sencilla.\n",
    "    - Rellenar con el valor más frecuente: despejado (representa el 75% de los datos), i.e.: solución con datos artificiales más sencilla.\n",
    "    - Rellenar con valores aleatorios según la proporción en la que aparecen los datos, i.e.: el 75% de los datos faltantes se rellenan arbitrariamente con \"Despejado\".\n",
    "\n",
    "    Lo que mejor preserva los datos es, rellenar con \"se desconoce\", pues la variable existe previamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5c1e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader.data['datasets/accidentalidad']['ESTADO_METEOROLÓGICO'] = data_loader.data['datasets/accidentalidad']['ESTADO_METEOROLÓGICO'].fillna(pd.Series(np.random.choice(['Despejado', 'Lluvia débil', 'Lluvia moderada', 'Lluvia fuerte', 'Granizo', 'Nieve', 'Niebla', 'Se desconoce'], p=[0.7, 0.1, 0.05, 0.05, 0.025, 0.025, 0.025, 0.025], size=len(data_loader.data['datasets/accidentalidad']))))\n",
    "data_loader.data['datasets/accidentalidad']['ESTADO_METEOROLÓGICO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19e895537b3fefc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_loader.data['datasets/accidentalidad']['ESTADO_METEOROLÓGICO'] = data_loader.data['datasets/accidentalidad']['ESTADO_METEOROLÓGICO'].fillna('Se desconoce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c3ed47a75517d2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- Tipo de vehículo. Hay 34 valores posibles: Ambulancia SAMUR, autobús EMT, autobús, autobús articulado, autobús articulado EMT, autocaravana, bicicleta, bicicleta EPAC (pedaleo asistido), camión de bomberos, camión rígido, ciclo, ciclomotor, ciclomotor de dos ruedas L1e-B, cuadriciclo ligero, cuadriciclo no ligero, furgoneta, maquinaria de obras, microbús <= 17 plazas, moto de tres ruedas > 125cc, moto de tres ruedas hasta 125cc, motocicleta > 125cc, motocicleta hasta 125cc, otros vehículos con motor, otros vehículos sin motor, patinete no eléctrico, remolque, semirremolque, sin especificar, todo terreno, tractocamión, tren/metro, turismo (68%), VMU eléctrico, vehículo articulado. En esta variable hay 0.6% de valores faltantes, lo cual no es significativo, i.e.: la sustitución que elijamos tendrá menos repercusión en el estudio final. Aquí, por lo tanto, hay 2 vías de actuación:\n",
    "    - Rellenar con \"sin especificar\", i.e.: solución sencilla y descriptiva pero que puede dar lugar a interpretaciones erróneas, pues puede haber sido otro tipo de vehículo que no se había registrado.\n",
    "    - Rellenar con el valor más probable según otro atributo (por ejemplo, código de lesividad).\n",
    "    \n",
    "    La solución más apropiada es rellenar con el valor más probable según código de lesividad, por lo que vamos a ver primero cómo se relacionan ambos atributos y después rellenaremos los valores faltantes con el valor más probable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cab20107357cf63",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_loader.data['datasets/accidentalidad']['TIPO_VEHICULO'] = data_loader.data['datasets/accidentalidad']['TIPO_VEHICULO'].fillna(data_loader.data['datasets/accidentalidad'].groupby('COD_LESIVIDAD')['TIPO_VEHICULO'].transform(lambda x:x.mode().iat[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1327182514854e40",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Para rellenar los datos de positivo en alcohol, usaremos lo más habitual según grupo de edad, sexo y lesividad de la persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5616191f4311771",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_loader.data['datasets/accidentalidad'][\"POSITIVA_ALCOHOL\"] = data_loader.data['datasets/accidentalidad'].groupby(['RANGO_EDAD','SEXO','LESIVIDAD'])['POSITIVA_ALCOHOL'].transform(lambda x: x.fillna(x.mode()[0] if not x.mode().empty                                                                                                        else \"Empty\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454c21c49c4303b3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Para determinar el tipo de persona, usaremos de criterio el rango de edad y la lesividad -si bien sólo faltan 3 datos, en esta situación podemos realizar un ajuste más específico-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1a47a31c2d409e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_loader.data['datasets/accidentalidad'][\"TIPO_PERSONA\"] = data_loader.data['datasets/accidentalidad'].groupby(['RANGO_EDAD','LESIVIDAD'])['TIPO_PERSONA'].transform(lambda x: x.fillna(x.mode()[0] if not x.mode().empty                                                                                                        else \"Empty\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fe4a56258fa644",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_loader.data['datasets/accidentalidad'] = data_loader.data['datasets/accidentalidad'][data_loader.data['datasets/accidentalidad'].DISTRITO != '13.0']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c389ad6fc3fa88",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Hacer una agrupación de los distritos de la zona norte y de la zona sur, calcular la desviación típica de las dos zonas y contar un poco lo que sale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9e50ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "accidentes = data_loader.data['datasets/accidentalidad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac736401",
   "metadata": {},
   "outputs": [],
   "source": [
    "accidentes['DISTRITO'] = accidentes['DISTRITO'].replace(['MORATALAZ', 'PUENTE-DE-VALLECAS', 'RETIRO', 'USERA', 'VILLA DE VALLECAS', 'VILLAVERDE', 'CARABANCHEL', 'VICÁLVARO', 'ARGANZUELA', 'CENTRO', 'LATINA', 'RETIRO'], 'SUR')\n",
    "accidentes['DISTRITO'] = accidentes['DISTRITO'].replace(['CIUDAD LINEAL', 'HORTALEZA', 'BARAJAS', 'SALAMANCA', 'SAN BLAS-CANILLEJAS', 'FUENCARRAL-EL-PARDO', 'MONCLOA-ARAVACA', 'CHAMARTíN', 'CHAMBERí', 'TETUáN'], 'NORTE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba11ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = accidentes.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3e79a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "numAccidentes = accidentes.groupby('DISTRITO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d20057",
   "metadata": {},
   "outputs": [],
   "source": [
    "std = numAccidentes.std()\n",
    "accidentes['DISTRITO']['NORTE'].value_counts().std()\n",
    "accidentes['DISTRITO']['SUR'].value_counts().std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8215e207",
   "metadata": {},
   "source": [
    "La desviación típica por distrito es 59640.214352398165, indica que hay una variabilidad alta en el número de accidentes entre cada distrito. Id est, el número de accidentes sube o baja 59640 accidentes respecto a la media (82838). Si separamos en zona norte y sur, vemos que hay una mayor dispersión de los datos en la zona sur que la zona norte (la desviación típica es mayor). Esto puede llevar a la conclusión de que es más fácil predecir la cantidad de accidentes que van a ocurrir en la zona norte que en la zona sur, aunque esto es lógico si también se tiene en cuenta que el número de accidentes en la zona sur es 3 veces la de la zona norte."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
