{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "403b90b536e46321",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Proyecto Open Data I\n",
    "## Radares, y su eficiencia en la CAM\n",
    "### Recopilación, limpieza y tratamiento de los datos\n",
    "Este cuaderno pretende enseñar el proceso de limpieza de los datos relativos a los radares en la CAM\n",
    "_Paula Gómez Lucas, Alejandro Majado Martínez_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Importar librerías\n",
    "import os\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ecfecdd16f381a8a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "A continuación, se muestra la clase que está compuesta de todos los métodos que se encargan de la limpieza y transformación de los datos"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32d82e29e7e3ba04"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CSVDataLoader:\n",
    "    \"\"\"\n",
    "    A class for loading and cleaning CSV data from a specified folder path.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    folder_path : str\n",
    "        The path to the folder containing the CSV files to be loaded.\n",
    "\n",
    "    data : dict\n",
    "        A dictionary containing the loaded CSV data, where the keys are the file names and the values are the corresponding dataframes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, folder_path):\n",
    "        \"\"\"\n",
    "        Initializes a CSVDataLoader object with the specified folder path.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        folder_path : str\n",
    "            The path to the folder containing the CSV files to be loaded.\n",
    "        \"\"\"\n",
    "        self.folder_path = folder_path\n",
    "        self.data = {}\n",
    "        self.filename = []\n",
    "        self.keys = []\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Loads CSV data from the specified folder path into a dictionary.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        None\n",
    "        \"\"\"\n",
    "        csv_files = [f for f in os.listdir(self.folder_path) if f.endswith('.csv')]\n",
    "        folders = (\"datasets/actuacionesBomberos\", \"datasets/estaciones\", \"datasets/accidentalidad\")\n",
    "        for folder in folders:\n",
    "            df = None\n",
    "            for file in os.listdir(folder):\n",
    "                filepath = folder + \"/\" + file\n",
    "                df1 = pd.read_csv(filepath, sep=';', encoding='utf-8', low_memory=False)\n",
    "                df = pd.concat([df, df1])\n",
    "            self.data[str(folder)] = df\n",
    "\n",
    "        for file_name in csv_files:\n",
    "            file_path = os.path.join(self.folder_path, file_name)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, sep=';', encoding='latin-1', low_memory=False)\n",
    "                self.data[str(file_name)] = df\n",
    "                self.filename.append(file_name)\n",
    "            except Exception as e:\n",
    "                print(f\"Error al leer {file_name}: {str(e)}\")\n",
    "\n",
    "        for value in self.data.keys():\n",
    "            self.keys.append(value)\n",
    "\n",
    "    def clean_data(self):\n",
    "        \"\"\"\n",
    "        Cleans the loaded CSV data by renaming columns, removing whitespace, dropping null values and duplicates, and converting date columns to datetime format.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        None\n",
    "        \"\"\"\n",
    "        columna_borrar = \"Unnamed\"\n",
    "        for df in self.data:\n",
    "            for j in self.data[df].columns:\n",
    "                if columna_borrar in j:\n",
    "                    while j in self.data[df].columns:\n",
    "                        self.data[df] = self.data[df].drop(j, axis=1)\n",
    "                        self.data[df] = self.data[df].dropna(how='all', axis=0)\n",
    "                        \n",
    "            self.data[df] = self.data[df].rename(columns = lambda x: x.strip().lower().replace(' ', '_'))\n",
    "            self.data[df] = self.data[df].map(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "            self.data[df] = self.data[df].dropna(how='all', axis=0)\n",
    "            self.data[df] = self.data[df].drop_duplicates()\n",
    "            self.data[df] = self.data[df].loc[:, ~self.data[df].columns.duplicated()]\n",
    "            self.data[df].columns = map(str.upper, self.data[df].columns)\n",
    "\n",
    "            if 'FECHA' in self.data[df].columns:\n",
    "                self.data[df]['FECHA'] = pd.to_datetime(self.data[df]['FECHA'], format='%d/%m/%Y')\n",
    "\n",
    "#           num_cols = self.data[i].select_dtypes(include='number').columns\n",
    "#           for col in num_cols:\n",
    "#               self.data[i][col] = self.data[i][col].fillna(self.data[i][col].mean())\n",
    "\n",
    "    def get_info(self, filename):\n",
    "        print(self.data[filename].isnull().sum())\n",
    "        print(self.data[filename].info())\n",
    "        \n",
    "    def get_nan_columns(self):\n",
    "        j = 0\n",
    "        for i in self.data:\n",
    "            print(self.keys[j])\n",
    "            self.get_info(i)\n",
    "            j+=1\n",
    "      \n",
    "    def get_cleaned_data(self):\n",
    "        \"\"\"\n",
    "        Returns the cleaned CSV data as a dictionary.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            A dictionary containing the cleaned CSV data, where the keys are the file names and the values are the corresponding dataframes.\n",
    "        \"\"\"\n",
    "        return self.data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bcff712b3750c172"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Una vez está definida la clase con sus métodos, procedemos a declarar las variables que nos permiten trabajar con ello"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b7335fcc8217616"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "folder_path = \"datasets\"\n",
    "data_loader = CSVDataLoader(folder_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbdf3ceea7c2a99b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "La siguiente función carga los datos de los csv a los dataframes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74f78fbede88c12c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_loader.load_data()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f582d432db7e08f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Limpiamos los datos eliminando las columnas autogeneradas con NaNs, renombramos las columnas para que sean uniformes (minúsculas y con barra bajas), eliminamos las filas de NaNs, eliminamos las filas duplicadas, formateamos todas las variables fecha para que sean consistentes (dd/mm/aaaa), sustituimos los NaNs de las variables numéricas con la media correspondiente a su variable."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d369c70da4759e50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_loader.clean_data()\n",
    "data = data_loader.get_cleaned_data()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86c9aa7d8bbe58f7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Por último, para confirmar que los datos se han cargado bien, utilizamos el método get_nan_columns para ver cuántos datos en cada columna quedan nulos, así como usamos el método info() de pandas para ver un resumen de todas las columnas y comprobamos también que todo el formateo de las columnas se ha realizado sin problema."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d56bd783adfada06"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_loader.get_nan_columns()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ce2dcc513c04e26"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Efectivamente, se realiza sin problemas. El siguiente paso es el análisis de los datasets columna a columna para revisar qué método usar para rellenar los datos faltantes dependiendo de cada atributo.  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "932b4983e737a46a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sustitución de valores faltantes\n",
    "Los datasets actuaciones bomberos y estaciones no tienen datos faltantes, por lo que sólo nos queda trabajar con los otros 8 datasets, que podemos observar aquí:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a0acbcdeb1e67e4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in data.keys():\n",
    "    print(i)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bedfa7b584c57ea6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Empecemos con accidentalidad: "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dcb6334a91e59ce1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_loader.get_info('datasets/accidentalidad')\n",
    "data_loader.data['datasets/accidentalidad']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f008d553841fa9aa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hay algunos datos faltantes que tiene sentido que lo sean, y podemos sustituir por un buzzword de algún tipo que nos haga saber que se trate de esto, como lo es que en un accidente en el que no se han producido lesiones, la lesividad sea nula y tampoco haya código de la misma, y como ambas cifras coinciden, es lógico pensar que se trata de las mismas situaciones. Podemos sustituir entonces todos los NaNs de Lesividad faltantes por 'Ninguna' y el código por -1.0."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a959b970c5daeeb6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_loader.data['datasets/accidentalidad']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45fc71c4fd83a506"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cf16ce8afdce10af"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Por otro lado, positivo en droga tiene valor sólo si daba positivo, por lo que rellenar los valores faltantes con 0 es lo más lógico (siendo 0 negativo en droga). "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7de03165e8d88e89"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6b09f9f02b03cb3f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Número, código de distrito, tipo de accidente, coordenadas (x e y), son atributos a los que sólo les falta un dato cada uno, por lo que no es representativo esta falta de datos y podemos rellenarlos con el valor más habitual. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6794df73ad915c7e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "295f5b9764e1d8dd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Por último, donde queda dilema es en positivo alcohol, tipo de vehículo y estado meteorológico. En esta situación, lo más apropiado es ver si, relacionando estos atributos con algún otro, es más probable que los atributos valgan uno u otro valor.\n",
    "\n",
    "- Estado meteorológico. Hay 7 valores posibles: despejado, lluvia débil, lluvia intensa, granizando, nevando, nublado, se desconoce. Aquí, por lo tanto, hay 3 vías de actuación:\n",
    "    - Rellenar con \"se desconoce\", i.e.: ser fieles a lo que se sabe, reducir la proporción de datos artificiales (hay un 11% de datos faltantes), solución sencilla.\n",
    "    - Rellenar con el valor más frecuente: despejado (representa el 75% de los datos), i.e.: solución con datos artificiales más sencilla.\n",
    "    - Rellenar con valores aleatorios según la proporción en la que aparecen los datos, i.e.: el 75% de los datos faltantes se rellenan arbitrariamente con \"Despejado\".\n",
    "\n",
    "    Lo que mejor preserva los datos es, rellenar con \"se desconoce\", pues la variable existe previamente."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f71acd44441657ed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Tipo de vehículo. Hay 34 valores posibles: Ambulancia SAMUR, autobús EMT, autobús, autobús articulado, autobús articulado EMT, autocaravana, bicicleta, bicicleta EPAC (pedaleo asistido), camión de bomberos, camión rígido, ciclo, ciclomotor, ciclomotor de dos ruedas L1e-B, cuadriciclo ligero, cuadriciclo no ligero, furgoneta, maquinaria de obras, microbús <= 17 plazas, moto de tres ruedas > 125cc, moto de tres ruedas hasta 125cc, motocicleta > 125cc, motocicleta hasta 125cc, otros vehículos con motor, otros vehículos sin motor, patinete no eléctrico, remolque, semirremolque, sin especificar, todo terreno, tractocamión, tren/metro, turismo (68%), VMU eléctrico, vehículo articulado. En esta variable hay 0.6% de valores faltantes, lo cual no es significativo, i.e.: la sustitución que elijamos tendrá menos repercusión en el estudio final. Aquí, por lo tanto, hay 2 vías de actuación:\n",
    "    - Rellenar con \"sin especificar\", i.e.: solución sencilla y descriptiva pero que puede dar lugar a interpretaciones erróneas, pues puede haber sido otro tipo de vehículo que no se había registrado.\n",
    "    - Rellenar con el valor más probable según otro atributo (por ejemplo, código de lesividad).\n",
    "    \n",
    "    La solución más apropiada es rellenar con el valor más probable según código de lesividad, por lo que vamos a ver primero cómo se relacionan ambos atributos y después rellenaremos los valores faltantes con el valor más probable."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "442d0a9ad6fd5f22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_loader.get_info('datasets/accidentalidad')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d1e9c91c6f7d2e4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Como podemos comprobar, no quedan datos faltantes. Pasamos a Actuaciones Bomberos:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "614d5b336df459ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_loader.get_info('datasets/actuacionesBomberos')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6be918428188fefd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Este dataset no tiene datos faltantes, pasamos al siguiente. Estaciones:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ef1ad3c5f5178c2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_loader.get_info('datasets/estaciones')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7550f545565b2ff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Este dataset tampoco tiene datos faltantes. Pasamos a la evolución histórica de las direcciones:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "616bb02cb7e81e21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_loader.get_info('DireccionesEvolucionHistorica_20231004.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c0e82a99f25c2fb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Como podemos observar, faltan datos de las columnas VIA_SQC (secuencia de la denominación), con 371584 NaNs; VIA_PAR (partícula de la denominación), con 17997; CALIFICADOR (del número de la policía), con 322826; FECHA_DE_BAJA (de la dirección), con 211657; y TIPO_NDP (tipo de número: portal, garaje, fachada, etc.) con 16248."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ffbe05631776336"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_loader.data['DireccionesEvolucionHistorica_20231004.csv']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c4d412d957bb13f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "La secuencia de denominación (VIA_SQC) es solo NaNs, por lo que lo más lógico es eliminar la columna directamente"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb0c6c1314a02704"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_loader.data['DireccionesEvolucionHistorica_20231004.csv'] = data_loader.data['DireccionesEvolucionHistorica_20231004.csv'].drop('VIA_SQC', axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b2d30caff708bb3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ahora, pasamos a la partícula de denominación (VIA_PAR), hay 7 valores distintos de entre la lista posible de 20 (DE, EL, A, DEL, LA, AL, DE LA, LAS, A LA, DE LAS, LO, A LAS, DE LO, LOS, A LO, DE LO, DE LOS, POR EL, A LOS, POR LA), de los cuales el primero es A LA y el último es DEL. Por lo tanto, parece que no se ha rellenado con Null o espacio vacío aquellas direcciones en las que no hay partícula:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c1db11e5b7dca5f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_loader.data['DireccionesEvolucionHistorica_20231004.csv']['VIA_PAR'] = data_loader.data['DireccionesEvolucionHistorica_20231004.csv']['VIA_PAR'].fillna(' ')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9ac22353dcb75e8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fecha de baja, si no hay valor, es que está aún de alta, así que también sustituimos por un valor que represente que sigue dado de alta:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71d668c5009d5907"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_loader.data['DireccionesEvolucionHistorica_20231004.csv']['FECHA_DE_BAJA'] = data_loader.data['DireccionesEvolucionHistorica_20231004.csv']['FECHA_DE_BAJA'].fillna('--/--/----')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f74b6982df068b0e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para Tipo de número de policía, rellenaremos con el dato más habitual: PORTAL"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a53da140048bda9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_loader.data['DireccionesEvolucionHistorica_20231004.csv']['TIPO_NDP'] = data_loader.data['DireccionesEvolucionHistorica_20231004.csv']['TIPO_NDP'].fillna(data_loader.data['DireccionesEvolucionHistorica_20231004.csv']['TIPO_NDP'].mode())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31dcf8adcf3d80a1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_loader.get_info('DireccionesEvolucionHistorica_20231004.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ff97a48355ee6b0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Efectivamente no quedan Missing Values. Pasamos a las direcciones vigentes:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc91bf1f3ffe60e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_loader.get_info('DireccionesVigentes_20231004.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28f6ca93fbbf8eea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Faltan valores en Vía Par (3381) y en Calificador (178838), los sustituimos con el mismo criterio que en el dataset anterior:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed1dce8b111e3840"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_loader.data['DireccionesVigentes_20231004.csv']['VIA_PAR'] = data_loader.data['DireccionesVigentes_20231004.csv']['VIA_PAR'].fillna(' ')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ff533ed980d126e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# mismo metodo"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3f7f2ac1191594c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_loader.get_info('DireccionesVigentes_20231004.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc403f7322f16562"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pasamos al dataset estrella: radares."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8acc23972e185c1a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_loader.get_info('RADARES_FIJOS_vDTT.csv')\n",
    "data_loader.data['RADARES_FIJOS_vDTT.csv']\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec0120bc1a8c715d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Tabla de frecuencia relativa de accidentes por distritos. En esta tabla se obtiene la frecuencia de accidentes en todos los distritos de madrid, esto hace que se pueda obtener también el tráfico de coches en el distrito. Aunque no sea una relación proporcional, a mayor número de accidentes supone un tráfico mayor en la zona."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c861d18cca0f1589"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "accidentes = data_loader.data['datasets/accidentalidad']\n",
    "frecuencia = accidentes['DISTRITO'].value_counts()\n",
    "accidentes_df = pd.DataFrame(frecuencia)\n",
    "accidentes_df.columns = ['Frecuencia absoluta']\n",
    "accidentes_df['Frecuencia relativa'] = 100*accidentes_df['Frecuencia absoluta'] / len(accidentes)\n",
    "frecuencia_rel_acumulada = accidentes_df['Frecuencia relativa'].cumsum()\n",
    "accidentes_df['Frecuencia relativa acumulada'] = frecuencia_rel_acumulada\n",
    "#por problemas de longitud de bytes, faltan decimales en la suma y no da el 100% exacto\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.set_title('Distribución de accidentes por distrito')\n",
    "ax.bar(accidentes_df.index, accidentes_df['Frecuencia absoluta'], color='blue')\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(accidentes_df.index, accidentes_df['Frecuencia relativa acumulada'], color='red', marker='o', ms = 5)\n",
    "ax2.yaxis.set_major_formatter(PercentFormatter())\n",
    "ax.tick_params(axis='y', color = 'blue')\n",
    "ax2.tick_params(axis='y', color = 'red')\n",
    "ax.set_xticklabels(accidentes_df.index, rotation=90)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71eb31a3f26d89ed"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
